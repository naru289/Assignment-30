{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naru289/Assignment-30/blob/main/M3_AST_30_Deep_Q_Learning_C%20Copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4Nwm4FK3wgU"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Assignment: Deep Q Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZbtiZbbHAOQ"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL6Uy59UHAOR"
      },
      "source": [
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* understand Q-learning\n",
        "* differentiate between Q-learning and Deep Q-learning\n",
        "* implement Deep Q-learning to solve Atari Breakout environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXBOfzUYHAOR"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJrTyoGDHAOS"
      },
      "source": [
        "**Q-Learning**\n",
        "\n",
        "The agent will perform the sequence of actions that will eventually generate the maximum total reward. This total reward is also called the **Q-value** and we will formalize our strategy as:\n",
        "\n",
        "$$Q(s, a) = r(s, a) + \\gamma\\ maxQ(s', a)$$\n",
        "\n",
        "The above equation states that the Q-value yielded from being at state $s$ and performing action $a$ is the immediate reward $r(s,a)$ plus the highest Q-value possible from the next state $s’$. Gamma here is the discount factor which controls the contribution of rewards further in the future.\n",
        "\n",
        "$Q(s’,a)$ depends on $Q(s”,a)$ which will then have a coefficient of gamma squared. So, the Q-value depends on Q-values of future states as shown here:\n",
        "\n",
        "$$Q(s, a) \\rightarrow \\gamma\\ Q(s', a) + \\gamma^2\\ Q(s'', a)\\ ...\\ ...\\ ...\\ \\gamma^n\\ Q(s''^{...n}, a)$$\n",
        "\n",
        "Adjusting the value of gamma will diminish or increase the contribution of future rewards.\n",
        "\n",
        "Since this is a recursive equation, we can start with making arbitrary assumptions for all q-values. With experience, it will converge to the optimal policy.\n",
        "\n",
        "To know more about Q-Learning, click [here](https://github.com/rishal-hurbans/Grokking-Artificial-Intelligence-Algorithms/tree/master/ch10-reinforcement_learning).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-TzAn8XHAOT"
      },
      "source": [
        "**Approximate Q-Learning and Deep Q-Learning**\n",
        "\n",
        "The main problem with Q-Learning is that it does not scale well to large (or even medium) Markov Decision Processes with many states and actions, and it is hard to keep track of an estimate for every single Q-Value.\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM-670x440.png\" width=650px />\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "The solution is to find a function $Q_θ(s, a)$ that approximates the Q-Value of any state-action pair (s, a) using a manageable number of parameters (given by the parameter vector θ). This is called **Approximate Q-Learning**.\n",
        "\n",
        "$$Q_{target}(s, a) = r + \\gamma\\ maxQ_{\\theta}(s', a')$$\n",
        "\n",
        "For years it was recommended to use linear combinations of handcrafted features extracted from the state to estimate Q-Values, but in 2013, DeepMind showed that deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a DQN for Approximate Q-Learning is called **Deep Q-Learning**.\n",
        "\n",
        "In deep Q-learning, we use a neural network to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr2ZG0TQHAOT"
      },
      "source": [
        "### Implementing Deep Q-Learning for Atari Breakout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4fqv6BuHAOU"
      },
      "source": [
        "Let's implement Deep Q-Learning on the Atari Breakout (`BreakoutNoFrameskip-v4`) environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aze2K918HAOU"
      },
      "source": [
        "**Atari Breakout**\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.gannett-cdn.com/media/USATODAY/USATODAY/2013/05/14/atari-breakout-16_9.jpg?width=1023&height=578&fit=crop&format=pjpg&auto=webp\" width=400px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "In this environment, a board moves along the bottom of the screen returning a ball that will destroy blocks at the top of the screen. The aim of the game is to remove all blocks and breakout of the level. The agent must learn to control the board by moving left and right, returning the ball and removing all the blocks without the ball passing the board."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vInRrxGKIu64",
        "outputId": "5ea89ba4-bd88-467d-f7ca-4a27fb71fb88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M3_AST_30_Deep_Q_Learning_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx pip3 install PyVirtualDisplay\")\n",
        "    ipython.magic(\"sx sudo apt-get install xvfb\")\n",
        "    ipython.magic(\"sx sudo apt-get install python-opengl\")\n",
        "    ipython.magic(\"sx sudo apt-get install ffmpeg\")\n",
        "    ipython.magic(\"sx pip install gym-notebook-wrapper\")\n",
        "    ipython.magic(\"sx pip install gym[atari]\")\n",
        "    ipython.magic(\"sx pip install gym[accept-rom-license]\")\n",
        "    ipython.magic(\"sx pip install pyglet\")\n",
        "    ipython.magic(\"sx sudo apt install freeglut3-dev freeglut3 libgl1-mesa-dev libglu1-mesa-dev libxext-dev libxt-dev\")\n",
        "    ipython.magic(\"sx sudo apt install python3-opengl libgl1-mesa-glx libglu1-mesa\")\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2583\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baUfsWTmHAOV"
      },
      "source": [
        "### Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fA3knP9nHAOW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import gnwrapper\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import urllib.request\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_IiZIFaHAOX"
      },
      "outputs": [],
      "source": [
        "# List of available environments\n",
        "from gym import envs\n",
        "print(envs.registry.all())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wJkENOhHAOY"
      },
      "source": [
        "### Configure parameters\n",
        "\n",
        "We will be using an epsilon-greedy algorithm for choosing the best action, where there is an epsilon chance of sampling a random action from the action space. Instead of using epsilon decay, we will be using linear annealing to decrease epsilon from 1 to 0.1 over 1 million frames by Deepmind’s specification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YArr9GLVHAOZ"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "# Discount factor for past rewards\n",
        "gamma = 0.99\n",
        "\n",
        "# Epsilon greedy parameter\n",
        "epsilon = 1.0\n",
        "\n",
        "# Minimum epsilon greedy parameter\n",
        "epsilon_min = 0.1\n",
        "\n",
        "# Maximum epsilon greedy parameter\n",
        "epsilon_max = 1.0\n",
        "\n",
        "# Rate at which to reduce chance of random action being taken\n",
        "epsilon_interval = (epsilon_max - epsilon_min)\n",
        "\n",
        "# Size of batch taken from replay buffer\n",
        "batch_size = 32\n",
        "\n",
        "# Number of frames to run\n",
        "max_steps_per_episode = 10000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ECYMkVVHAOZ"
      },
      "source": [
        "Next, we define functions used to show the video by adding it to the CoLab notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1ddymg9HAOZ"
      },
      "outputs": [],
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\" Utility functions to enable video recording of gym environment and displaying it.\n",
        "To enable video, we just do \"env = wrap_env(env) \"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else:\n",
        "    print(\"Could not find video\")\n",
        "\n",
        "\n",
        "def wrap_env(env):\n",
        "    try:\n",
        "        env = gnwrapper.Monitor(env, './video', \"recording\")\n",
        "    except:\n",
        "        env = gnwrapper.Monitor(env, './video', \"recording\")\n",
        "    return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3pDVsJiHAOa"
      },
      "source": [
        "### Instantiate the environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PlrslBIxHAOa"
      },
      "outputs": [],
      "source": [
        "# Create Breakout environment\n",
        "# Initialize the environment ‘Breakout-v5’.\n",
        "env = wrap_env(gym.make(\"ALE/Breakout-v5\"))\n",
        "env.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can ignore the above runtime warinings (if any)"
      ],
      "metadata": {
        "id": "j-eELy4cg8Se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " It is possible to specify various flavors of the environment via the keyword arguments` difficulty `and `mode`. A flavor is a combination of a game mode and a difficulty setting.\n",
        "\n",
        "\n",
        " You may use the suffix \"-ram\" to switch to the RAM observation space. In v0 and v4, the suffixes \"Deterministic\" and \"NoFrameskip\" are available. These are no longer supported in v5. In order to obtain equivalent behavior, pass keyword arguments to gym.make as outlined in the general article on Atari environments. The versions v0 and v4 are not contained in the \"ALE\" namespace. I.e. they are instantiated via gym.make(\"Breakout-v0\")."
      ],
      "metadata": {
        "id": "gii_1NsBo1F1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Version History**\n",
        "\n",
        "*   v5: Stickiness was added back and stochastic frameskipping was removed. The entire action space is used by default. The environments are now in the “ALE” namespace.\n",
        "\n",
        "*   v4: Stickiness of actions was removed\n",
        "\n",
        "*   v0: Initial versions release (1.0.0)\n",
        "\n",
        "\n",
        "Here is the [Atari Breakout](https://www.gymlibrary.dev/environments/atari/breakout/) reference document\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MSQFVuAqpIDy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ghrwPUX1AfF"
      },
      "source": [
        "The format that gym takes is a concatenated string of `[‘Game][‘NoFrameskip’, ‘Deterministic’, None][‘-v0’, ‘-v4’]`. Here’s a quick explanation of each term:\n",
        "\n",
        "\n",
        "*   “NoFrameskip”: Each step of the environment is one frame\n",
        "\n",
        "*   “Deterministic”: Each step executes the same action for k frames and returns the kth frame. k = 4.\n",
        "\n",
        "*   None: Same as “Deterministic” but k is sampled from [2, 5].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF8vcN0rTTJV"
      },
      "outputs": [],
      "source": [
        "print('State shape: ', env.observation_space.shape)\n",
        "print('Number of actions: ', env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laKtW9JAHAOa"
      },
      "source": [
        "### Create the Deep Q-Network Model\n",
        "\n",
        "Deep Q network learns an approximation of the Q-table, which is a mapping between the states and actions that an agent will take. For every state in Breakout environment, we'll have four actions that can be taken:\n",
        "\n",
        "* **0:** do nothing\n",
        "* **1:** fire ball to start game\n",
        "* **2:** move right\n",
        "* **3:** move left\n",
        "\n",
        "The environment provides the state, and the action is chosen by selecting the larger of the four Q-values predicted in the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-aZkypvHAOa"
      },
      "outputs": [],
      "source": [
        "# Resets the environment to an initial state and returns an initial observation.\n",
        "# Shape of observation or state\n",
        "env.reset().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AolxuOcj9q4p"
      },
      "source": [
        "Refer to the following [Deepmind paper](https://arxiv.org/pdf/1312.5602v1.pdf) for playing Atari with Deep Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgATLM--HAOb"
      },
      "outputs": [],
      "source": [
        "# Write a funtion for model creation\n",
        "num_actions = 4\n",
        "def create_q_model():\n",
        "\n",
        "    # Network defined by the Deepmind paper\n",
        "    inputs = layers.Input(shape=(210, 160, 3))\n",
        "\n",
        "    # Convolutions on the frames on the screen\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t4n9Me9HAOb"
      },
      "source": [
        "The first model makes the predictions for Q-values which are used to make an action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fos3rgMLHAOb"
      },
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_q_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99cNxGbnHAOb"
      },
      "source": [
        "Now build a target model for the prediction of future rewards. Since the same network is calculating the predicted value and the target value, there could be a lot of divergence between these two. So, instead of using one neural network for learning, we can use two.\n",
        "\n",
        "The weights of a target model get updated every 10000 steps thus when the loss between the Q-values is calculated the target Q-value is stable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2Er6a13HAOc"
      },
      "outputs": [],
      "source": [
        "# Create target model\n",
        "model_target = create_q_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IO5H_7DHAOc"
      },
      "source": [
        "### Train the Model\n",
        "\n",
        "The following pseudo-algorithm implements deep Q-learning with experience replay.\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/deep_Q_learning.png\" width=480px, height=480px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eibJgnoYU31l"
      },
      "source": [
        "**Note:** The below code cell might take some time to run, suggesting to use GPU. Refer to the following [link](https://towardsdatascience.com/reinforcement-learning-explained-visually-part-5-deep-q-networks-step-by-step-5a5317197f4b) for the more details of Training the Deep-Q Networks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyglet.gl import *"
      ],
      "metadata": {
        "id": "Fh_Dl0X08Bqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvZAXnP96Vmt"
      },
      "outputs": [],
      "source": [
        "# In the Deepmind paper they use RMSProp however then Adam optimizer\n",
        "# improves training time\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Experience replay buffers\n",
        "# Reinforcement learning algorithms use replay buffers to store trajectories of experience\n",
        "# when executing a policy in an environment. During training, replay buffers are queried for a\n",
        "# subset of the trajectories (either a sequential subset or a sample) to \"replay\" the agent's experience\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 50000\n",
        "\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "\n",
        "# Maximum replay length\n",
        "max_memory_length = 100000\n",
        "\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 4\n",
        "\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "\n",
        "# Using huber loss for stability and also to avoid exploding gradients\n",
        "# Huber loss is a combination of linear as well as quadratic scoring methods.\n",
        "# It has an additional hyperparameter delta (δ). Loss is linear for values above delta and quadratic below delta.\n",
        "# Compared with MSE, Huber Loss is less sensitive to outliers as if the loss is too much\n",
        "# it changes quadratic equation to linear and hence is a combination of both MSE and MAE.\n",
        "loss_function = keras.losses.Huber()\n",
        "\n",
        "while True:  # Run until solved\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    # episodes - This indicates how many games we want the agent to play in order to train itself\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        frame_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration to select an action\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # With the probability epsilon, we take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            # with probability 1-epsilon, we select an action that has a maximum Q-value\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        # Hence, a decaying epsilon ensures that our agent does not rely upon the\n",
        "        # random predictions at the initial training epochs, only to later on exploit\n",
        "        # its own predictions more aggressively as the Q-function converges to more consistent predictions.\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Is used to display the environment image\n",
        "        env.render()\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        # env.step - executes the given action and returns four values\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor([float(done_history[i]) for i in indices])\n",
        "\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            # The Target network takes the next state from each data sample and predicts\n",
        "            # the best (max predicted Q value) out of all actions that can be taken from that state. This is the ‘Target Q Value’\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            # Compute Q value\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(future_rewards, axis=1)\n",
        "\n",
        "            # If final frame set the last value to -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                # Train the model on the states and updated Q-values\n",
        "                # The Q network takes the current state and action from each data sample\n",
        "                # and predicts the Q value for that particular action. This is the ‘Predicted Q Value’.\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "\n",
        "                # Calculate loss between new Q-value and old Q-value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            # after we compute the loss using the given loss function, and we use the tape to compute\n",
        "            # the gradient of the loss with regard to the model’s trainable variables. Again,\n",
        "            # these gradients will be tweaked later, before we apply them, depending on how good or bad the action turned out to be.\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    # Condition to consider the task solved\n",
        "    # Note that this execution may take more than 30 minutes\n",
        "    if running_reward > 2.4 or episode_count >100:\n",
        "        print(\"Stopped at episode {}!\".format(episode_count))\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K58TLcolHAOe"
      },
      "source": [
        "**Here**, one of the stopping criteria is if `running_reward > 2.4`, by increasing this value, learning can be improved. Here, we have chosen the `episode_count > 100` by increasing this value the training time also increases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Note:** The Deepmind paper trained for \"a total of 50 milion frames( that is 38 days of game experience in total)\". However it will give good results at around 10 million frames which are processed in less than 24 hours on a modern machine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILgJHu0WFQo0"
      },
      "source": [
        "### Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlniR4lFHAOg"
      },
      "outputs": [],
      "source": [
        "# Visualize training\n",
        "env.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nbb0FmW1HAOg"
      },
      "source": [
        "We can also get a gist of how the learning processes through the figures given below.\n",
        "\n",
        "Before any training:\n",
        "\n",
        "<img src=\"https://i.imgur.com/rRxXF4H.gif\" />\n",
        "\n",
        "In early stages of training:\n",
        "\n",
        "<img src=\"https://i.imgur.com/X8ghdpL.gif\" />\n",
        "\n",
        "In later stages of training:\n",
        "\n",
        "<img src=\"https://i.imgur.com/Z1K6qBQ.gif\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibp8o1S_Gmp0"
      },
      "source": [
        "Consider the statements given below and answer Q1.\n",
        "\n",
        "A. A Q-table is a simple data structure that we use to keep track of the states, actions, and their expected rewards. More specifically, the Q-table maps a state-action pair to a Q-value (the estimated optimal future value) which the agent will learn.\n",
        "\n",
        "B. A Q-table is a data structure used to calculate the minimum expected future rewards for the action at each state.\n",
        "\n",
        "C. In Deep Q-Learning, the regular Q-table can be replaced with either neural network or MLP or CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "-INhmnH1RDq0"
      },
      "outputs": [],
      "source": [
        "#@title Q.1. Which of the above statements is/are False?\n",
        "Answer1 = \"Only A\" #@param [\"\",\"Only A\",\"Both A & B\",\"Only B\", \"Only C\", \"Both B & C\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "RXPPf27bMG01"
      },
      "outputs": [],
      "source": [
        "#@title Q.2. In DQNs (Deep Q-Networks), the Experience buffer(replay buffer) is used to store past experiences in memory and replay some of them by randomly sampling from a uniform distribution. Randomly sampled training data from the replay buffer provides an identical and independent distributions for training the DQN. Thus, it helps avoid possible sampling bias to improve learning performance.\n",
        "Answer2 = \"TRUE\" #@param [\"\",\"TRUE\", \"FALSE\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "00ab3d48-7e49-4f74-c4b1-2d2330b9b79c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2583\n",
            "Date of submission:  02 Sep 2023\n",
            "Time of submission:  21:09:38\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}